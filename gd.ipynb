import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
def J(x, y, h_theta):
    m = len(x)
    return (1/(2*m)) * np.sum((y - h_theta)**2)
def dJ_theta_1(x, y, h_theta):
    m = len(x)
    return -(1/m) * np.sum(x * (y - h_theta))
def dJ_theta_0(x, y, h_theta):
    m = len(x)
    return -(1/m) * np.sum(y - h_theta)
def gradient_descent(x, y):
    iterations = 100
    theta_0 = 0
    theta_1 = 0
    learning_rate = 0.8
    costs = []
    thetas_0 = []
    thetas_1 = []
    for i in range(iterations):
        h_theta = theta_0 + theta_1 * x
        cost = J(x, y, h_theta)
        md = dJ_theta_1(x, y, h_theta)
        bd = dJ_theta_0(x, y, h_theta)
        theta_0 -= bd * learning_rate
        theta_1 -= md * learning_rate
        costs.append(cost)
        thetas_0.append(theta_0)
        thetas_1.append(theta_1)
        print("m {}, b {}, cost {} iteration {}".format(theta_1, theta_0, cost, i))
    return thetas_0, thetas_1, costs, theta_0, theta_1
# x=[0.23993,0.80287,0.104615,0.30075,0.53666,0.08107,0.18509,0.96605,0.32419,0.38713]
# y=[2.81443,3.30876,0.41829,-0.37122,2022992,1.76152,2.84154,1.15965,0.29688,-0.13069]
x = np.random.randn(10, 1)
y = 2 * x + np.random.randn(10, 1)
# indices = np.argsort(x, axis=-1)
# x = np.take_along_axis(x, indices, axis=-1)
# y = np.take_along_axis(y, indices, axis=1)
thetas_0, thetas_1, costs, final_theta_0, final_theta_1 = gradient_descent(x, y)
fig = plt.figure(figsize=(12, 5))
ax1 = fig.add_subplot(121, projection='3d')
theta_0_vals = np.linspace(-1, 3, 100)
theta_1_vals = np.linspace(-1, 3, 100)
theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)
cost_vals = np.array([J(x, y, theta_0 + theta_1 * x) for theta_0, theta_1 in zip(theta_0_vals.flatten(), theta_1_vals.flatten())])
cost_vals = cost_vals.reshape(theta_0_vals.shape)
ax1.plot_surface(theta_0_vals, theta_1_vals, cost_vals, cmap='viridis', alpha=0.8)
ax1.set_xlabel('Theta 0')
ax1.set_ylabel('Theta 1')
ax1.set_zlabel('Cost')
ax1.scatter(thetas_0, thetas_1, costs, color='r', marker='o', label='Gradient Descent')
ax2 = fig.add_subplot(122)
ax2.scatter(x, y, label='Data')
fit_line = final_theta_0 + final_theta_1 * x
ax2.plot(x, fit_line, color='r', label='Fitted Line')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.legend()
plt.show()


# convex function definition:
# A convex function is a mathematical function where the line segment joining any two points on its graph lies above or coincides with the graph itself,
# indicating that the function's values increase in a consistent manner along any interval within its domain.
